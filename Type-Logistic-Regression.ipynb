{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression \n",
    "We will use 1 vs all to train eighteen different logistic regression models with L2 regularization.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "\n",
    "#TODO \n",
    "#read in data \n",
    "trainX = np.loadtxt('spamtrainX.data')\n",
    "trainY = np.loadtxt('spamtrainY.data')[:,np.newaxis]\n",
    "testX = np.loadtxt('spamtestX.data')\n",
    "testY = np.loadtxt('spamtestY.data')[:,np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss, gradient of the loss, and Hessian of the loss for logistic regression.\n",
    "\n",
    "\\begin{align*}\n",
    "L &= \\left[\\sum_{i=1}^m \\ln(1+e^{-y_ix_i^\\top w})\\right] + \\frac{\\lambda}{2}\\sum_{j=2}^n w_j^2\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "p_i &= \\sigma(y_ix_i^\\top w) \\\\\n",
    "\\tilde{w} &= \\begin{bmatrix} 0 & w_2 & w_3 & \\dots & w_n \\end{bmatrix}^\\top \\\\\n",
    "\\tilde{I} &= \\begin{bmatrix} 0 & 0 & 0 & & 0 \\\\\n",
    "                        0 & 1 & 0 & \\dots & 0 \\\\\n",
    "                        0 & 0 & 1 & & 0 \\\\\n",
    "                        & \\vdots & & \\ddots & \\vdots \\\\\n",
    "                        0 & 0 & 0 & \\dots & 1 \\end{bmatrix} \\\\\n",
    "\\nabla L &= - \\left[\\sum_{i=1}^m (1-p_i)y_i x_i^\\top\\right] + \\lambda\\tilde{w} \\\\\n",
    "H_L &= \\left[\\sum_{i=1}^m p_i(1-p_i)x_ix_i^\\top\\right] + \\lambda\\tilde{I}\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code to calculate the same here\n",
    "\n",
    "def lrloss(w,X,Y,lmbda):\n",
    "    wnob = w.copy()\n",
    "    wnob[0] = 0\n",
    "    return (np.sum(np.log(1+np.exp(-Y*(X@w))),0).T + 0.5*lmbda*np.sum(wnob*wnob))[0]\n",
    "\n",
    "def lrgrad(w,X,Y,lmbda):\n",
    "    wnob = w.copy()\n",
    "    wnob[0] = 0\n",
    "    return -np.sum((1-1/(1+np.exp(-Y*(X@w))))*X*Y,0)[:,np.newaxis] + lmbda*wnob\n",
    "\n",
    "def lrhess(w,X,Y,lmbda):\n",
    "    p = 1/(1+np.exp(-Y*(X@w)))\n",
    "    ey = np.eye(w.shape[0])\n",
    "    ey[0,0] = 0\n",
    "    R = p*(1-p)\n",
    "    return X.T@np.diag(R[:,0])@X + lmbda*ey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newton-Raphson minimization.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient descent implemented with a constant step size\n",
    "# (note: this is *not* a good implementation, but just to show the idea)\n",
    "# The \"ittfn\" can help with debugging, but isn't necessary\n",
    "def graddesc(w,eta,fn,gradfn,ittfn=None):\n",
    "    oldf = fn(w)\n",
    "    df = 1\n",
    "    while(df>1e-6):\n",
    "        w = w - eta*gradfn(w)\n",
    "        newf = fn(w)\n",
    "        df = oldf-newf # hope to be positive, or we've over-shot and will be done\n",
    "        if ittfn is not None:\n",
    "            ittfn(w,eta,newf)\n",
    "        oldf = newf\n",
    "    return w\n",
    "    \n",
    "# Newton-Raphson minimization\n",
    "def newton(w,fn,gradfn,hessfn,ittfn=None):\n",
    "    oldf = fn(w)\n",
    "    eta = 1\n",
    "    while True:\n",
    "        g = gradfn(w)\n",
    "        H = hessfn(w)\n",
    "        neww = w - np.linalg.solve(H,g)\n",
    "        newf = fn(neww)\n",
    "        if newf>=oldf:\n",
    "            usedg = True\n",
    "            eta *= 2\n",
    "            while eta>1e-10:\n",
    "                neww = w - eta*g\n",
    "                newf = fn(neww)\n",
    "                if newf<oldf:\n",
    "                    break\n",
    "                eta *= 0.5\n",
    "            if eta<1e-10:\n",
    "                return w\n",
    "        else:\n",
    "            usedg = False\n",
    "        oldf = newf\n",
    "        w = neww\n",
    "        if ittfn is not None:\n",
    "            if usedg:\n",
    "                ittfn(w,eta,oldf)\n",
    "            else:\n",
    "                ittfn(w,0,oldf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and check error rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainlr(X,Y,lmbda):\n",
    "    w0 = np.zeros((X.shape[1],1)) # starting w at zero works well for LR\n",
    "    return newton(w0,lambda w : lrloss(w,X,Y,lmbda),\n",
    "                  lambda w : lrgrad(w,X,Y,lmbda),\n",
    "                  lambda w : lrhess(w,X,Y,lmbda))\n",
    "\n",
    "def lrerrorrate(X,Y,w):\n",
    "    return np.sum(Y*X@w<0)/Y.shape[0]\n",
    "\n",
    "lmbda = 0.1\n",
    "useextrafeatures = True\n",
    "\n",
    "myw = trainlr(mytrainX,trainY,lmbda)\n",
    "#myw = (myw>1.0).astype(float)\n",
    "#print (myw.T)\n",
    "print (lrerrorrate(mytestX,testY,myw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3-fold cross validation\n",
    "Do not need to do this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xvalideval(X,Y,lmbda,extrafeatures):\n",
    "    nfold = 3\n",
    "    tX = phi(X,extrafeatures)\n",
    "    def evalfn(X,Y,eX,eY):\n",
    "        return lrerrorrate(eX,eY,trainlr(X,Y,lmbda))\n",
    "        \n",
    "    m = X.shape[0]\n",
    "    splits = np.linspace(0,m,nfold+1).astype(int)\n",
    "    v = 0\n",
    "    for low,high in zip(splits[0:-1],splits[1:]):\n",
    "        trainX = np.vstack((tX[:low,:],tX[high:,:]))\n",
    "        trainY = np.vstack((Y[:low,:],Y[high:,:]))\n",
    "        validX = tX[low:high,:]\n",
    "        validY = Y[low:high,:]\n",
    "        v +=evalfn(trainX,trainY,validX,validY)\n",
    "    return v\n",
    "\n",
    "ls = np.logspace(-4,2,10)\n",
    "xverr = ls.copy()\n",
    "bstv = None\n",
    "bste = None\n",
    "bstl = None\n",
    "for i,l in enumerate(ls):\n",
    "    xverr[i] = xvalideval(trainX,trainY,l,False)\n",
    "    if bstv is None or bstv>xverr[i]:\n",
    "        bstv = xverr[i]\n",
    "        bstl = l\n",
    "        bste = False\n",
    "plt.semilogx(ls,xverr,'b-')\n",
    "for i,l in enumerate(ls):\n",
    "    xverr[i] = xvalideval(trainX,trainY,l,True)\n",
    "    if bstv is None or bstv>xverr[i]:\n",
    "        bstv = xverr[i]\n",
    "        bstl = l\n",
    "        bste = True\n",
    "plt.semilogx(ls,xverr,'g-')\n",
    "plt.legend(['raw features','raw + binary features'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
